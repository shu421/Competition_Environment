{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e532851-2afb-45d7-83f2-b862a9adef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append('./utils/')\n",
    "import utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "from abc import abstractmethod\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "import optuna.integration.lightgbm as lgbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0ebc696-5a67-4c40-8a8b-c2e499cbfd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "817d2e0d-945f-4e77-9c09-feb226454335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_Model(object):\n",
    "    @abstractmethod\n",
    "    def fit(self, train_x, train_y, valid_x, valid_y):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @abstractmethod\n",
    "    def predict(self, model, features):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def cv(self, train_y, train_features, test_features, fold_ids, is_reg=True): # is_reg=True:回帰, False:分類\n",
    "        test_preds = np.zeros(len(test_features))\n",
    "        oof_preds = np.zeros(len(traon_features))\n",
    "        if(is_reg==False):\n",
    "            test_preds = pd.DataFrame()\n",
    "            \n",
    "        for i_fold, (trn_idx, val_idx) in enumerate(fold_ids):\n",
    "            \n",
    "            trn_x = train_features.iloc[trn_idx]\n",
    "            val_x = train_features.iloc[val_idx]\n",
    "            trn_y = train_y.iloc[trn_idx]\n",
    "            val_y = train_y.iloc[val_idx]\n",
    "            \n",
    "            model = self.fit(trn_x, trn_y, val_x, val_y)\n",
    "        \n",
    "            oof_preds[val_idx] = self.predict(model, val_x)\n",
    "            \n",
    "            # 回帰\n",
    "            if(is_reg):\n",
    "                oof_score = np.sqrt(mean_squared_error(val_y, oof_preds[val_idx]))\n",
    "                print(f'Fold{i_fold}_RMSE : {oof_score}')\n",
    "                test_preds += self.predict(model, test_features)/len(fold_ids)\n",
    "            # 分類\n",
    "            else:\n",
    "                oof_score = f1_score(val_y, np.round(oof_preds[val_idx]), average='macro') # 予測確率をroundで四捨五入\n",
    "                print(f'Fold{i_fold}_F1 : {oof_score}')\n",
    "                test_preds[f'Fold{i_fold}'] = self.predict(model, test_features)\n",
    "                \n",
    "        if(is_reg):\n",
    "            oof_score = np.sqrt(mean_squared_error(train_y, oof_preds))\n",
    "            print('-'*50)\n",
    "            print(f'oof score : {oof_score}')\n",
    "            print('-'*50)\n",
    "        else:\n",
    "            oof_score = f1_score(train_y, np.round(oof_preds), average='macro')\n",
    "            print('-'*50)\n",
    "            print(f'oof score : {oof_score}')\n",
    "            print('-'*50)\n",
    "            test_preds = test_preds.T.mode().loc[0] # mode:最頻値\n",
    "        \n",
    "        evals_results = {'evals_result':{\n",
    "            'oof_score':oof_score,\n",
    "            'n_data':len(train_features),\n",
    "            'n_features':len(train_features.columns),\n",
    "        }}\n",
    "        \n",
    "        return oof_preds, test_preds, evals_results\n",
    "    \n",
    "    \n",
    "    \n",
    "class Lgbm(Base_Model):\n",
    "    def __init__(self, model_params):\n",
    "        self.model_params = model_params\n",
    "        self.models = []\n",
    "        self.feature_cols = None\n",
    "        \n",
    "    def fit(self, train_x, train_y, valid_x, valid_y):\n",
    "        lgb_train = lgb.Dataset(train_x, train_y)\n",
    "        lgb_valid = lgb.Dataset(valid_x, valid_y)\n",
    "        \n",
    "        model = lgb.train(self.model_params,\n",
    "                         train_set=lgb_train,\n",
    "                         valid_sets=[lgb_valid],\n",
    "                         valid_names=['valid'],\n",
    "                         early_stopping_rounds=100,\n",
    "                         num_boost_round=99999,\n",
    "                         verbose_eval=300,\n",
    "                         #categorical_features=cat_col\n",
    "                         )\n",
    "        self.models.append(model)\n",
    "        return model\n",
    "    \n",
    "    def predict(self, model, features):\n",
    "        self.feature_cols = features.columns\n",
    "        return np.argmax(model.predict(features), axis=1)\n",
    "    \n",
    "    def tuning(self, y, X, params, cv):\n",
    "        lgbo_train = lgbo.dataset(X, y)\n",
    "        \n",
    "        tuner_cv = lgbo.LightGBMTunerCV(\n",
    "        params,\n",
    "        lgbo_train,\n",
    "        num_boost_round=99999,\n",
    "        verbose_eval=300,\n",
    "        folds=cv,\n",
    "        #categorical_feature=cat_col\n",
    "        )\n",
    "        \n",
    "        tuner_cv.run()\n",
    "        print('----------------------------------------------')\n",
    "        print('Best_score:',tuner_cv.best_score)\n",
    "        print('Best_params:',tuner_cv.best_params)\n",
    "        print('-----------------------------------------------')\n",
    "        return tuner_cv.best_params\n",
    "    \n",
    "    \n",
    "    def visualize_importance(self):\n",
    "        feature_importance_df = pd.DataFrame()\n",
    "\n",
    "        for i,model in enumerate(self.models):\n",
    "            _df = pd.DataFrame()\n",
    "            _df['feature_importance'] = model.feature_importance(importance_type='gain')\n",
    "            _df['column'] = self.feature_cols\n",
    "            _df['fold'] = i+1\n",
    "            feature_importance_df = pd.concat([feature_importance_df,_df],axis=0,ignore_index=True)\n",
    "        \n",
    "        order = feature_importance_df.groupby('column').sum()[['feature_importance']].sort_values('feature_importance',ascending=False).index[:50]\n",
    "\n",
    "        fig, ax = plt.subplots(2,1,figsize=(max(6, len(order) * .4), 14))\n",
    "        sns.boxenplot(data=feature_importance_df, x='column', y='feature_importance', order=order, ax=ax[0], palette='viridis')\n",
    "        ax[0].tick_params(axis='x', rotation=90)\n",
    "        ax[0].grid()\n",
    "        fig.tight_layout()\n",
    "        return fig,ax\n",
    "    \n",
    "    \n",
    "    def save(self, path):\n",
    "        utils.Util.dump(self.model, path)\n",
    "        \n",
    "    def load(self, path):\n",
    "        self.model = utils.Util.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d74cd-e2e9-42f5-b1aa-6913abcd66e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
